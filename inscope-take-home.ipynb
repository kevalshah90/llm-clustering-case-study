{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87f6f523",
   "metadata": {},
   "source": [
    "### Inscope Take Home\n",
    "\n",
    "In this assignment you will build a prototype of a cluster analysis tool to navigate financial statements.\n",
    "\n",
    "Each company has a unique CIK (Central Index Key) that is used to identify it in the data. The CIK is a 10 digit number, and is the prefix of the file name for each company's 10-K report.\n",
    "\n",
    "There are three main tasks in this assignment:\n",
    "\n",
    "- To construct a CIK -> Company Name mapping from the provided data.\n",
    "\n",
    "- To summarize each company's report into a few sentences.\n",
    "\n",
    "- To cluster the companies into similar groups based on their financial statements.\n",
    "\n",
    "The goal of this assignment is to demonstrate your ability to build a data pipeline to process unstructured data, and to use that data to build a simple clustering and summarizing tool whose output could be built into a more complex application. What we expect you to build are proofs of concept, and not production-ready models.\n",
    "\n",
    "**Specification** \n",
    "\n",
    "- [x] You will filter down the dataset to cluster companies that are in the S&P 500 index. You can find a recent list of CIKs for companies in the S&P 500 in the SP500.txt file.\n",
    "\n",
    "\n",
    "- [x] You will create a script that given a directory with report files can produce a CIK -> Company Name mapping in the shape of a CSV file with two columns: CIK and Company Name. Each row in this file will represent each file in the provided data. (hint: you don't need to throw an LLM at this problem)\n",
    "\n",
    "\n",
    "- [x] You will run your mapping script on the provided data, and include it in your response.\n",
    "\n",
    "\n",
    "- [x] You will write a data pipeline to process the provided HTML into an intermediate representation that can be used for clustering. One of the features in your intermediate representation should be a 1-paragraph summary of the report. You can use any pre-trained language model you like to generate the summary.\n",
    " \n",
    " \n",
    "- [x] You will use your pipeline to assign every company in the dataset into similar groups based on their financial statements.\n",
    "\n",
    "\n",
    "- [x] You will provide a Jupyter Notebook, a Streamlit app, or equivalent for users to inspect and interact with the results of your clustering and summarization. The visualization should allow the user to select a company and show other similar companies in the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1295db5f",
   "metadata": {},
   "source": [
    "#### Task #1 - Creating a CIK - Company Name mapping script\n",
    "\n",
    "- You will filter down the dataset to cluster companies that are in the S&P 500 index. You can find a recent list of CIKs for companies in the S&P 500 in the SP500.txt file.\n",
    "\n",
    "- You will create a script that given a directory with report files can produce a CIK -> Company Name mapping in the shape of a CSV file with two columns: CIK and Company Name. Each row in this file will represent each file in the provided data. (hint: you don't need to throw an LLM at this problem)\n",
    "\n",
    "- You will run your mapping script on the provided data, and include it in your response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "76c8cf23-f8ad-4f48-b1f2-5530012d4cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain\n",
    "#!pip install torch\n",
    "#!pip install transformers\n",
    "#!pip install bs4\n",
    "#!pip install sentence_transformers\n",
    "#!pip install openai\n",
    "#!pip install faiss-gpu\n",
    "#!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "60ec9274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import langchain\n",
    "import torch\n",
    "import transformers\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import BertTokenizer, BertModel, pipeline\n",
    "import sentencepiece\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1d5bc723",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = \"xxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cabae4",
   "metadata": {},
   "source": [
    "Read files from the directory and filter to keep only S&P500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "004ff65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify directory path\n",
    "dir_path = os.getcwd() + '/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "590ca500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the files from the directory\n",
    "orig_files = [f for f in os.listdir(dir_path)]\n",
    "len(orig_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6f49b4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0001722482.html',\n",
       " '0001317839.html',\n",
       " '0001141234.html',\n",
       " '0000320193.html',\n",
       " '0001408210.html']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "eadae6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0001722482', '0001317839', '0001141234', '0000320193', '0001408210']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean up for matching\n",
    "files = [f.replace(\".html\",\"\") for f in orig_files if \".html\" in f]\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0f49234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a pandas dataframe\n",
    "#df = pd.DataFrame({'cos_idx': files})\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "96a3af4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0000909832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0000723254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0001013462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0000060667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0000906345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           col1\n",
       "131  0000909832\n",
       "109  0000723254\n",
       "41   0001013462\n",
       "289  0000060667\n",
       "83   0000906345"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read S&P 500 companies data \n",
    "sp500 = pd.read_csv('SP500.txt', delimiter=\"\\t\", header=None, names = ['col1'], dtype=str) # read as string, so that leading zeros are not truncated\n",
    "sp500.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "89a35aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502, 1)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "666ac699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499, 1)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop duplicates\n",
    "sp500 = sp500.drop_duplicates(subset=\"col1\")\n",
    "sp500.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d27064b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0000091142', '0000001800', '0001551152', '0000815094', '0001467373']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To List\n",
    "sp500_lst = list(sp500['col1'])\n",
    "sp500_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2ed07363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtered list of 3000+ to S&P500\n",
    "filtered_list = [f for f in files if f in sp500_lst]\n",
    "len(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b45018df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CIK - Company ticket mapping file from sec.gov\n",
    "with open('sec_co.json', 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "sec_co = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a10f0882-ad9e-4450-aa19-c1ef84f20453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000320193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001048286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001048695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000079879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000100517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cik\n",
       "0  0000320193\n",
       "1  0001048286\n",
       "2  0001048695\n",
       "3  0000079879\n",
       "4  0000100517"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe with filtered list of companies in S&P500\n",
    "df_cik_co = pd.DataFrame({'cik': filtered_list})\n",
    "df_cik_co.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28827acb-5b91-468f-887b-9c5730eec8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function of lookup company name \n",
    "def cik_company(cik):\n",
    "\n",
    "    # match cik format, remove leading 0s\n",
    "    cik = int(cik)\n",
    "\n",
    "    for co in sec_co.values():\n",
    "\n",
    "        if co['cik_str'] == cik:\n",
    "            return co['title']\n",
    "            \n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98a30f40-93c4-4c31-93c9-35e07f59d474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>company_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000320193</td>\n",
       "      <td>Apple Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001048286</td>\n",
       "      <td>MARRIOTT INTERNATIONAL INC /MD/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001048695</td>\n",
       "      <td>F5, INC.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000079879</td>\n",
       "      <td>PPG INDUSTRIES INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000100517</td>\n",
       "      <td>United Airlines Holdings, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0000945841</td>\n",
       "      <td>POOL CORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0000019617</td>\n",
       "      <td>JPMORGAN CHASE &amp; CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0001410636</td>\n",
       "      <td>American Water Works Company, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0001725057</td>\n",
       "      <td>Ceridian HCM Holding Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0001373715</td>\n",
       "      <td>ServiceNow, Inc.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>497 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            cik                        company_name\n",
       "0    0000320193                          Apple Inc.\n",
       "1    0001048286     MARRIOTT INTERNATIONAL INC /MD/\n",
       "2    0001048695                            F5, INC.\n",
       "3    0000079879                  PPG INDUSTRIES INC\n",
       "4    0000100517      United Airlines Holdings, Inc.\n",
       "..          ...                                 ...\n",
       "492  0000945841                           POOL CORP\n",
       "493  0000019617                 JPMORGAN CHASE & CO\n",
       "494  0001410636  American Water Works Company, Inc.\n",
       "495  0001725057           Ceridian HCM Holding Inc.\n",
       "496  0001373715                    ServiceNow, Inc.\n",
       "\n",
       "[497 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply function on each row of pandas dataframe\n",
    "df_cik_co['company_name'] = df_cik_co.apply(lambda x: cik_company(x['cik']), axis=1)\n",
    "df_cik_co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47065a18-ac20-4855-92e8-aea491ec3448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the mapping table in a CSV\n",
    "df_cik_co.to_csv(os.getcwd() + \"/data/df_cik_co.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba3636d-b9b7-4706-b31c-9df4b98ea077",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011a900-5431-4d1c-94f0-dc2351ac7735",
   "metadata": {},
   "source": [
    "#### Task #2 - Clustering + Summarization\n",
    "\n",
    "- You will write a data pipeline to process the provided HTML into an intermediate representation that can be used for clustering. One of the features in your intermediate representation should be a 1-paragraph summary of the report. You can use any pre-trained language model you like to generate the summary.\n",
    "\n",
    "- You will use your pipeline to assign every company in the dataset into similar groups based on their financial statements.\n",
    "  \n",
    "- You will provide a Jupyter Notebook, a Streamlit app, or equivalent for users to inspect and interact with the results of your clustering and summarization. The visualization should allow the user to select a company and show other similar companies in the same cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1380d20f-1a5b-4c21-8901-83c2d7870ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code to parse html text and clean up\n",
    "file = \"0001051470.html\"\n",
    "\n",
    "with open(os.getcwd() + '/data/{}'.format(file), 'r') as file:\n",
    "    \n",
    "    content = file.read()\n",
    "        \n",
    "    # parse html\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "    #text = soup.get_text()\n",
    "\n",
    "    #header_text = soup.find_all([\"h5\"])\n",
    "    para_text = soup.get_text()\n",
    "\n",
    "    #print('text', para_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "099f6607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip the initial pages - table of contents\n",
    "text_to_summarize = para_text\n",
    "ts = text_to_summarize[25000:]\n",
    "#ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2cbe2e",
   "metadata": {},
   "source": [
    "Using `t5-small` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c90c74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\n",
    "                        task=\"summarization\",\n",
    "                        model=\"t5-small\",\n",
    "                        min_length=50,\n",
    "                        max_length=500,\n",
    "                        truncation=True\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "de2964f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'FORM 10-K __________________________ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934For the transition period from to Commission File Number 001-16441 . Yes  No Indicate by check mark if the registrant is not required to file reports pursuant to Section 13 or Section 15d of the Act .'}]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e689b7ee",
   "metadata": {},
   "source": [
    "Summaries are not very meaningful or cohesive. They look like simple extractive summaries of the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa42044",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f91b919",
   "metadata": {},
   "source": [
    "Using `langchain` document loaders + LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b208ac4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text  FORM 10-K  __________________________☒ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934For the fiscal year ended December 31, 2022 or ☐TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934For the transition period from              to             Commission File Number 001-16441  __________________________CROWN CASTLE INC. (Exact name of registrant as specified in its charter) __________________________ Delaware 76-0470458(State or other jurisdictionof incorporation or organization) (I.R.S. EmployerIdentification No.)8020 Katy Freeway, Houston, Texas 77024-1908(Address of principal executive offices) (Zip Code)(713) 570-3000 (Registrant's telephone number, including area code) Securities Registered Pursuant toSection 12(b) of the ActTrading SymbolsName of Each Exchangeon Which RegisteredCommon Stock, $0.01 par valueCCINew York Stock ExchangeSecurities Registered Pursuant to Section 12(g) of the Act: NONE. ______________________________________Indicate by check mark if the registrant is a well-known seasoned issuer, as defined in Rule 405 of the Securities Act.    Yes  ☒    No  ☐Indicate by check mark if the registrant is not required to file reports pursuant to Section 13 or Section 15(d) of the Act.    Yes  ☐    No  ☒Indicate by check mark whether the registrant (1) has filed all reports required to be filed by Section 13 or 15(d) of the Securities Exchange Act of 1934 during the preceding 12 months (or for such shorter period that the registrant was required to file such reports), and (2) has been subject to such filing requirements for the past 90 days.    Yes  ☒    No  ☐Indicate by check mark whether the registrant has submitted electronically every Interactive Data File required to be submitted pursuant to Rule 405 of Regulation S-T (§232.405 of this chapter) during the preceding 12 months (or for such shorter period that the registrant was required to submit such files).    Yes  ☒    No  ☐Indicate by check mark whether the registrant is a large accelerated filer, an accelerated filer, a non-accelerated filer, a smaller reporting company, or an emerging growth company. See definitions of a \"large accelerated filer,\" \"accelerated filer,\" \"smaller reporting company,\" and \"emerging growth company\" in Rule 12b-2 of the Exchange Act.Large accelerated filer   ☒    Accelerated filer  ☐    Non-accelerated filer  ☐  Smaller reporting company  ☐ Emerging growth company  ☐ If an emerging growth company, indicate by check mark if the registrant has elected not to use the extended transition period for complying with any new or revised financial accounting standards provided pursuant to Section 13(a) of the Exchange Act   ☐ Indicate by check mark whether the registrant has filed a report on and attestation to its management's assessment of the effectiveness of its internal control over financial reporting under Section 404(b) of the Sarbanes-Oxley Act (15 U.S.C. 7262(b)) by the registered public accounting firm that prepared or issued its audit report.   ☒If securities are registered pursuant to Section 12(b) of the Act, indicate by check mark whether the financial statements of the registrant included in the filing reflect the correction of an error to previously issued financial statements. ☐Indicate by check mark whether any of those error corrections are restatements that required a recovery analysis of incentive-based compensation received by any of the registrant’s executive officers during the relevant recovery period pursuant to §240.10D-1(b). ☐Indicate by check mark whether the registrant is a shell company (as defined in Rule 12b-2 of the Act).    Yes  ☐    No  ☒The aggregate market value of the voting and non-voting common equity held by non-affiliates of the registrant was approximately $72.6 billion as of June 30, 2022, the last business day of the registrant's most recently completed second fiscal quarter, based on the New York Stock Exchange closing price on that day of $168.38 per share.Applicable Only to Corporate RegistrantsAs of February 21, 2023, there were 433,437,494 shares of common stock outstanding.Documents Incorporated by ReferenceThe information required to be furnished pursuant to Part III of this Form 10-K will be set forth in, and incorporated by reference from, the registrant's definitive proxy statement for the annual meeting of stockholders (\"2023 Proxy Statement\"), which will be filed with the Securities and Exchange Commission not later than 120 days after the end of the fiscal year ended December 31, 2022.CROWN CASTLE INC.TABLE OF CONTENTS   PagePART IItem 1.Business4Item 1A.Risk Factors12Item 1B.Unresolved Staff Comments24Item 2.Properties24Item 3.Legal Proceedings24Item 4.Mine Safety Disclosures24PART IIItem 5.Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities25Item 6.[Reserved]27Item 7.Management's Discussion and Analysis of Financial Condition and \n"
     ]
    }
   ],
   "source": [
    "# test code to parse html text and clean up\n",
    "file = \"0001051470.html\"\n",
    "\n",
    "with open(os.getcwd() + '/data/{}'.format(file), 'r') as file:\n",
    "    \n",
    "    content = file.read()\n",
    "        \n",
    "    # parse html\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    " \n",
    "    para_text = soup.get_text()\n",
    "\n",
    "    # skip initial pages - table of content\n",
    "    ts = para_text[23000:]\n",
    "    print('text', ts[0:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "474d1b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model='gpt-3.5-turbo-16k', \n",
    "    request_timeout=120\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6988af96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crown Castle Inc. is a company that owns, operates, and leases shared communications infrastructure in the United States. This includes towers, small cells, and fiber assets. The company's core business is providing access to its communications infrastructure through long-term tenant contracts. They aim to grow cash flows from their existing infrastructure, return cash to stockholders in the form of dividends, and invest capital to grow cash flows and dividends per share. The company faces competition from other infrastructure providers and is subject to federal, state, and local regulations. The demand for their infrastructure is driven by the increasing demand for data and the growth of wireless networks. However, any slowdown in demand or reduction in network investment by their tenants could negatively impact the company.\n"
     ]
    }
   ],
   "source": [
    "llm_summary = llm.invoke(f'Summarize: \"{ts[0:50000]}\"')\n",
    "print(llm_summary.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e771155",
   "metadata": {},
   "source": [
    "Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6aa10be2-3163-449f-9799-982ba0562ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Function to read .html file and parse the text\n",
    "\n",
    "2. Tokenize \n",
    "\n",
    "3. Generate Embeddings\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class doc_function:\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def parse_html(self, cik):\n",
    "\n",
    "        print('cik', cik)\n",
    "        \n",
    "        with open(os.getcwd() + '/data/{}.html'.format(cik), 'r') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "            # parse html\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "            all_texts = soup.get_text()\n",
    "\n",
    "            # skip initial pages - table of content\n",
    "            texts = all_texts[23000:]\n",
    "\n",
    "            return texts\n",
    "\n",
    "    def summarize(self, parsed_text):\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            llm = ChatOpenAI(\n",
    "                             model=self.model, \n",
    "                             request_timeout=120\n",
    "                            )\n",
    "\n",
    "            # pass the snippet to get produce summary, limit to max tokens allowed for the model\n",
    "            llm_summary = llm.invoke(f'Summarize: \"{parsed_text[0:16384]}\"')\n",
    "\n",
    "            print('Summary length:', len(llm_summary.content))\n",
    "\n",
    "            return llm_summary.content\n",
    "        \n",
    "        except Exception as e:\n",
    "            print('Exception', e)\n",
    "            return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "81d3b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the class\n",
    "model = 'gpt-3.5-turbo-16k'\n",
    "doc_func = doc_function(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3f70474e-f0ad-4087-ba7e-1099292efbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parsed html text\n",
    "df_cik_co['html_text'] = df_cik_co.apply(lambda x: doc_func.parse_html(x['cik']), axis=1)\n",
    "df_cik_co.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1ea00a74-731a-4565-97b1-eefe50e20c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save parsed html texts\n",
    "#df_cik_co.to_csv(os.getcwd() + '/data/df_cik_html_parsed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "110cefed-ee67-46b7-8e98-3682ac069644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries\n",
    "df_cik_co['summary'] = df_cik_co.apply(lambda x: doc_func.summarize(x['html_text']), axis=1)\n",
    "df_cik_co.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2b5d3829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save summaries\n",
    "#df_cik_co.to_csv(os.getcwd() + '/data/df_cik_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4128b3e3",
   "metadata": {},
   "source": [
    "#### Clutering using FAISS: https://github.com/facebookresearch/faiss\n",
    "\n",
    "Steps involved: \n",
    "\n",
    "1. Vectors / embeddings \n",
    "2. Create a FAISS index for storage and search\n",
    "3. Implement k-means or other clustering algorithms\n",
    "4. Assign clusters\n",
    "5. Predict cluster of unseen text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "72be619a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b4b30693744b9b98814fa761811072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a pre-trained BERT model for text embeddings\n",
    "bert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Obtain BERT embeddings for the document text\n",
    "text_data = df_cik_co['html_text']\n",
    "embeddings = bert_model.encode(text_data, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2b20e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FAISS index\n",
    "d = embeddings.shape[1]  # Dimension of the embeddings\n",
    "index = faiss.IndexFlatL2(d) \n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ae3664bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "48b85a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering using k-means\n",
    "nclusters = 10\n",
    "kmeans = faiss.Kmeans(d, nclusters)\n",
    "kmeans.train(embeddings)\n",
    "cluster_assign = kmeans.assign(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "8d5f4071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster assignments to the DataFrame\n",
    "df_cik_co[\"cluster\"] = cluster_assign[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "75938ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final dataframe with clusters\n",
    "df_cik_co.to_csv(os.getcwd() + \"/App/df_cik_final.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c95616",
   "metadata": {},
   "source": [
    "### Follow-up questions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1275c1",
   "metadata": {},
   "source": [
    "- Describe which task you found most difficult in the implementation, and why.\n",
    "\n",
    "> Parsing text from the `.html` files and subsequently generating summaries of the document text was challenging. Given the layout of the document, different structure containing disclaimers, table of content, it was tricky to extract clean text / information that we are interested in. \n",
    "\n",
    "> Upon parsing the html, the document text is fairly length and exceeds the context window allowed LLMs for summarization. We are not able to pass the complete text for summarization and need to experiment with different chunking strategies, extractive, abstractive outputs and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519871ec",
   "metadata": {},
   "source": [
    "- What led you to choose the libraries or frameworks you used in your implementation?\n",
    "\n",
    "> I ended up choosing `langchain's chatOpenAI` module given it's simplicity to to work with openAI models. `AWS SageMaker` for compute and processing power. `gpt-3.5-turbo-16k` for larger context window. `FAISS` for vector storage and search functionality and support for clustering algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69197e46",
   "metadata": {},
   "source": [
    "- How did you evaluate whether the clusters and summaries created by your system were good or not?\n",
    "\n",
    "> In the absence of ground-truth data, one way to evaluate is randomly sample companies from the list and compare the system-generated summaries against human-generated ones. \n",
    "\n",
    "> For clustering, I compared the companies within each cluster based on their industry / sector, financial performance, regulations and so on. For example; cluster #3 includes financial companies like Morgan Stanley, BlackRock, PayPal, Bank of America, Goldman Sachs and JPMorgan Chase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb7ec3",
   "metadata": {},
   "source": [
    "- If there were no time or budget restrictions for this exercise, what improvements would you make in the following areas:\n",
    "\n",
    "    > Implementation\n",
    "    \n",
    "       I'd spend more time dealing with unstructured data and extracting relevant text from the 10k's. I'd experiment with different chunking strategies and summarization methods based on the use-case. For example, chunk based on sections in the report or each paragraph? Generate summaries of the each chunk of the document and then combine them for a more cohesive summary. Try out a few different models with larger context window. \n",
    "        \n",
    "        Try an domain specific model or Fine-tune one on financial data and utilize that for generating summaries. Example, https://huggingface.co/ProsusAI/finbert.   \n",
    "    \n",
    "    > User Experience\n",
    "    \n",
    "       I'd work on generating more meaningful clusters. The improvements here would be from extracting relevant texts, generating embeddings and then trying out different clustering / similarity algorithms.\n",
    "    \n",
    "    > Data Quality\n",
    "    \n",
    "       Improvements in how we process unstructured textual data. Using Beautiful soup to parse text, identify and clean up irrelevant texts, piping in factual data such as financials, sector, market cap and so on and using them for either feature engineering for classical ML or into the embeddings for LLMs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc2897",
   "metadata": {},
   "source": [
    "- If you built this using classic ML models, how would approach it if you had to build it with LLMs? Similarly, if you used LLMs, what are some things you would try if you had to build it with classic ML models?\n",
    "\n",
    "> For classical ML model, I'd attempt to process unstructured text into structured data by extracting relevant data points for each company for feature engineering. Data points could include financial information such as revenue, profits, expenses, operational costs, assets, liabilities, employees, market / sector, competition and so on. I'd then train an unsupervised clustering algorithm based on features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758b0b7b",
   "metadata": {},
   "source": [
    "- If you had to build this as part of a production system, providing an inference API for previously unseen reports, how would you do it? What would you change in your implementation?\n",
    "\n",
    "> For production systems, I'd implement some the improvements around data quality, UX and evaluations outlined above. Once we have satisfactory results, I'd productionize using `AWS SageMaker` for deploying the model to production and creating an inference endpoint. I'd consider different inference options (batch, real-time, async), costs, latency, throughput, other metrics for running a production system. I'd ensure proper configuration of logs, uptime, downtime to monitor usage. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
